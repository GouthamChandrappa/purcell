# -*- coding: utf-8 -*-
"""image_classficiation_vgg16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10JxrjKyvBz5a_emqMbJ0MKjeYcj2KXY0
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
mount = '/content/drive/'
drive.mount(mount)
!pwd

import os
drive_root = mount + "/My Drive/Colab Notebooks/purcell/throat"

print("\nColab: Changing directory to ", drive_root)
# %cd $drive_root

# from google.colab import drive
# drive.mount('/content/drive')

!pwd

base_dir = "./"
train_dir = os.path.join( base_dir, 'train')
validation_dir = os.path.join( base_dir, 'validation')




train_strep_dir = os.path.join(train_dir, 'pharyngitis') # Directory with our training unhealthy pictures
train_healthy_dir = os.path.join(train_dir, 'no_pharyngitis') # Directory with our training healthy pictures
validation_strep_dir = os.path.join(validation_dir, 'pharyngitis') # Directory with our validation unhealthy pictures
validation_healthy_dir = os.path.join(validation_dir, 'no_pharyngitis')# Directory with our validation healthy pictures

train_strep_fnames = os.listdir(train_strep_dir)
train_healthy_fnames = os.listdir(train_healthy_dir)

validation_strep_fnames = os.listdir(validation_strep_dir)
validation_healthy_fnames = os.listdir(validation_healthy_dir)

validation_strep_fnames[0]

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.applications import VGG16

from tensorflow.keras.preprocessing.image import ImageDataGenerator


# Add our data-augmentation parameters to ImageDataGenerator
# train_datagen = ImageDataGenerator(rescale = 1./255.,
#                                    rotation_range = 40,
#                                    width_shift_range = 0.2,
#                                    height_shift_range = 0.2,
#                                    shear_range = 0.2,
#                                    zoom_range = 0.2,
#                                    horizontal_flip = True)

train_datagen = ImageDataGenerator(rescale = 1.0/255.,
                                   rotation_range = 40,
                                   horizontal_flip = True)

# Note that the validation data should not be augmented!
test_datagen = ImageDataGenerator( rescale = 1.0/255. )

# Flow training images in batches of 20 using train_datagen generator
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    batch_size = 20,
                                                    shuffle=False,
                                                    class_mode = 'categorical',
                                                    target_size = (256, 256))

# Flow validation images in batches of 20 using test_datagen generator
validation_generator =  test_datagen.flow_from_directory( validation_dir,
                                                          batch_size  = 10,
                                                          shuffle=False,
                                                          class_mode  = 'categorical',
                                                          target_size = (256, 256))

inp = keras.layers.Input(shape=(256,256,3))
vggtest = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,
                                            input_shape=(256,256,3))

vggtest.summary()

def getbase_model(inp):
  vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,
                                            input_shape=(256,256,3))
  vgg.trainable = False
  return vgg

def myvggmodel():
  inp = keras.layers.Input(shape=(256,256,3))
  # vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,
  #                                           input_shape=(256,256,3))
  # vgg.trainable = False

  # for layers in vgg.layers[14:]:
  #   layers.trainable = True
    # print(layers.name, "-", layers.trainable)
  vgg = getbase_model(inp)
  x = vgg.get_layer('block5_pool').output
  x = tf.keras.layers.GlobalAveragePooling2D()(x)
  x = keras.layers.Dense(64, activation='relu')(x)
  output = keras.layers.Dense(2, activation='sigmoid')(x)

  model = tf.keras.models.Model(inputs = inp, outputs=output)

  return model

# inp = keras.layers.Input(shape=(224,224,3))
# vgg = tf.keras.applications.VGG16(include_top=True, weights='imagenet', input_tensor=inp,
#                                             input_shape=(256,256,3))
# vgg.trainable = False
# vgg.summary()

myvggmodel().summary()

keras.backend.clear_session()
model = myvggmodel()
model.summary()

i = 1
for layers in model.layers:
  print(i,layers.name, "-", layers.trainable)
  i= i+1



#### CHANGE The TESTING LAYERS
# for layers in model.layers[14:]:
#   layers.trainable = True
#   print(layers.name, "-", layers.trainable)

i=1
for layers in model.layers:
  print(i,layers.name, layers.trainable)
  i = i +1



############### model Compile #############
# ### Default
model.compile('adam', 'categorical_crossentropy', ['acc'])

# from tensorflow.keras.optimizers import Adam


# base_learning_rate = 0.001
# model.compile(optimizer = Adam(learning_rate=base_learning_rate),
#               loss = 'categorical_crossentropy',
#               metrics = ['acc'])

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

import cv2

!pip uninstall wandb

!pip install wandb

import wandb

# Check if 'wandb' is a module
import types

if isinstance(wandb, types.ModuleType):
    print("wandb is a module, you can proceed with wandb.login()")
else:
    print("wandb is not a module. You might have overwritten it.")
    print("Try restarting the kernel and then run wandb.login() again.")

# After restarting the kernel, try logging in again:
# import wandb
# wandb.login()
####KEY:  ff983137809568d214c2b3c63ad5a7acb7e8eb32

wandb.login()

from wandb.integration.keras import WandbCallback

class GradCAM:
    """
    Reference:
        https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/
    """

    def __init__(self, model, layerName):
        self.model = model
        self.layerName = layerName

        self.gradModel = tf.keras.models.Model(inputs=[self.model.inputs],
                                               outputs=[self.model.get_layer(self.layerName).output, self.model.output])

    def compute_heatmap(self, image, classIdx, eps=1e-8):
        with tf.GradientTape() as tape:
            tape.watch(self.gradModel.get_layer(self.layerName).variables)
            inputs = tf.cast(image, tf.float32)
            (convOutputs, predictions) = self.gradModel(inputs)

            if len(predictions) == 1:
                # Binary Classification
                loss = predictions[0]
            else:
                loss = predictions[:, classIdx]

        grads = tape.gradient(loss, convOutputs)

        castConvOutputs = tf.cast(convOutputs > 0, "float32")
        castGrads = tf.cast(grads > 0, "float32")
        guidedGrads = castConvOutputs * castGrads * grads

        convOutputs = convOutputs[0]
        guidedGrads = guidedGrads[0]

        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))
        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)

        (w, h) = (image.shape[2], image.shape[1])
        heatmap = cv2.resize(cam.numpy(), (w, h))

        numer = heatmap - np.min(heatmap)
        denom = (heatmap.max() - heatmap.min()) + eps
        heatmap = numer / denom
        heatmap = (heatmap * 255).astype("uint8")

        return heatmap


    def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_VIRIDIS):
        heatmap = cv2.applyColorMap(heatmap, colormap)
        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)

        return (heatmap, output)

class GRADCamLogger(tf.keras.callbacks.Callback):
    def __init__(self, validation_data, layer_name):
      super(GRADCamLogger, self).__init__()
      self.validation_data = validation_data
      self.layer_name = layer_name

    def on_epoch_end(self, logs, epoch):
      images = []
      grad_cam = []

      ## Initialize GRADCam Class
      cam = GradCAM(model, self.layer_name)

      for image in self.validation_data:
        image = np.expand_dims(image, 0)
        pred = model.predict(image)
        classIDx = np.argmax(pred[0])
        print("predict Label:", classIDx)

        ## Compute Heatmap
        heatmap = cam.compute_heatmap(image, classIDx)

        image = image.reshape(image.shape[1:])
        image = image*255
        image = image.astype(np.uint8)

        ## Overlay heatmap on original image
        heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1]))
        (heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5)

        images.append(image)
        grad_cam.append(output)

      wandb.log({"images": [wandb.Image(image)
                            for image in images]})
      wandb.log({"gradcam": [wandb.Image(cam)
                            for cam in grad_cam]})

sample_images, sample_labels = validation_generator[4]
sample_images.shape, sample_labels.shape

sample_labels

wandb.init(project="throat_activation_map")

history = model.fit(train_generator,
                    steps_per_epoch=10,
                    epochs=20,
                    validation_data=validation_generator,
                    validation_steps=5)

vgg = getbase_model()
vgg.trainable = False
for layers in vgg.layers[14:]:
  layers.trainable = True
  print(layers.name, "-", layers.trainable)

i=1
for layers in vgg.layers:
  print(i,layers.name, layers.trainable)
  i = i +1

model.compile(optimizer = Adam(learning_rate=base_learning_rate/10),
              loss = 'categorical_crossentropy',
              metrics = ['acc'])

history = model.fit(train_generator,
                    steps_per_epoch=10,
                    epochs=20,
                    validation_data=validation_generator,
                    validation_steps=5,
                    callbacks=[WandbCallback(input_type="image", validation_data=(sample_images, sample_labels)),
                               GRADCamLogger(sample_images, layer_name='block5_conv3')])

### Model run with TPU 6m39 s

##### Load model

mymodel =keras.models.load_model("./wandb/run-20240730_184227-xi1oa5j9/files/model-best.h5")

sample_images[9].shape
sample_labels[9]

# img_input = sample_images[9].reshape(256,256,3)
# input = cv2.resize(img_input, (256, 256))
# input = np.expand_dims(input, axis=0)



cam = GradCAM(model, 'block5_conv3')

image = np.expand_dims(sample_images[9], 0)
pred = model.predict(image)
print(pred)
classIDx = np.argmax(pred[0])
print("predict Label:", classIDx)



## Compute Heatmap
heatmap = cam.compute_heatmap(image, classIDx)

image = image.reshape(image.shape[1:])
image = image*255
image = image.astype(np.uint8)

        ## Overlay heatmap on original image
heatmap = cv2.resize(heatmap, (image.shape[0],image.shape[1]))
### edge detection


(heatmap, output) = cam.overlay_heatmap(heatmap, image, alpha=0.5)

#### without Classification #####
import numpy as np
import matplotlib.pyplot as plt
import cv2

image = np.expand_dims(sample_images[9], 0)

image = image.reshape(image.shape[1:])
image = image*255
image = image.astype(np.uint8)

image.shape

img_edge = cv2.Laplacian(image, cv2.CV_8U, ksize=5)
plt.matshow(img_edge)
plt.title('Laplacian')
plt.show()
# Grey Scale
# img_grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


from google.colab.patches import cv2_imshow
img_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
cv2_imshow( img_gray)
cv2.waitKey(0)
cv2.destroyAllWindows()



#Remove Noise- Blavl tp blck - white - white
img_lp_im_grey = cv2.cvtColor(img_edge, cv2.COLOR_BGR2GRAY)
plt.matshow(img_edge)
plt.title('Remove Noise')
plt.show()


## Remove additional Noise
blur_im = cv2.GaussianBlur(img_lp_im_grey, (5, 5), 0)

## apply threshhold
_, tresh_im = cv2.threshold(blur_im, 245, 255,cv2.THRESH_BINARY +  cv2.THRESH_OTSU)
# Invert Black and white
inverted_original = cv2.subtract(255, tresh_im)

plt.matshow(inverted_original)
plt.title('Inver black and White')
plt.show()

# plt.matshow(image)
# plt.show()

import matplotlib.pyplot as plt

plt.matshow(output)
plt.show()

images.append(image)
grad_cam.append(output)

for i in validation_generator:
    check = 50000
    # idx = (validation_generator.batch_index - 1) * validation_generator.batch_size
    # print(validation_generator.filenames[idx : idx + validation_generator.batch_size])
    #print(i, i[0])
    # print("Image shape", i[0].shape)
    # print("Label shape", i[1].shape)

    if check == 50000:
      print("Image shape", i[0].shape)
      print("Label shape", i[1].shape)
      print("Label ", i[1])
      break

validation_generator.index_array

validation_generator.filepaths[10:]

validation_generator.batch_index -1
validation_generator.filepaths

a[0].shape

from matplotlib import image
import matplotlib.pyplot as plt
import numpy as np

plt.imshow(a[0].reshape(256,256,3))
plt.show()

b[0]

model.summary()

# model.compile(optimizer = RMSprop(learning_rate=0.0001),
#               loss = 'categorical_crossentropy',
#               metrics = ['acc'])




model.compile(optimizer = RMSprop(learning_rate=0.0001),
              loss = 'categorical_crossentropy',
              metrics = ['val_acc'])

import tensorflow.keras as keras

checkpoint_filepath = './incepv3_model.keras'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='acc',
    mode='max',
    verbose = 1,
    save_best_only=True)

################ VISUAIZATION ##################

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

import cv2

"""INSTALL WANDB"""

!ls

model.summary()





sample_images, sample_labels = validation_generator[4]
sample_images.shape, sample_labels.shape

from wandb.integration.keras import WandbCallback

history = model.fit(train_generator,
                    steps_per_epoch=10,
                    epochs=20,
                    validation_data=validation_generator,
                    validation_steps=5,
                    callbacks=[WandbCallback(input_type="image", validation_data=(sample_images, sample_labels)),
                               GRADCamLogger(sample_images, layer_name='conv2d_69')])

import tensorflow as tf
from tensorflow import keras



from matplotlib import image
import matplotlib.pyplot as plt
import cv2

import numpy as np



mymodel =keras.models.load_model("./wandb/run-20240711_004520-bo0px2bt/files/model-best.h5")



mymodel.summary()

!pwd

validation_data_check = (sample_images, sample_labels)

sample_images.shape, sample_labels.shape

i = 0
for im in validation_data_check:
  print("count:"i,":",im.shape)
  i+ =1

np.argmax(sample_labels[0])

sample_images.shape

#sample_images[0].shape
# mygradModel = tf.keras.models.Model(inputs=[model.inputs],
#                                                outputs=[model.get_layer(layerName).output, model.output])

for i in range(sample_images.shape[0]):
  input1 = cv2.resize(sample_images[i], (256, 256))
  input = np.expand_dims(input1, axis=0)
  preds = mymodel.predict(input)

  print(preds)
  cId = np.argmax(preds[0])
  print(cId)
  print("length of prediction" , len(preds[0]))

  print("TRUTH:", np.argmax(sample_labels[i]))

import matplotlib.pyplot as plt

for i in range(sample_images.shape[0]):
  plt.title(np.argmax(sample_labels[i]))
  # plt.imshow(sample_images[i].reshape(256,256,3))
  # plt.show()

sample_labels

a = [0.96666, 0.878788]
a = np.array(a)
np.argmax(a)
np.argmax(a)

import numpy as np
import matplotlib.pyplot as plt
import cv2

sample_labels

print(i)
  i += 1
  print(imageTest.shape)
  print(imageTest[6].shape)
  # for j in imageTest:
  #   image = np.expand_dims(imageTest[j], 0)
  #   print(image.shape)

  # pred = mymodel.predict(image)
  # classIDx = np.argmax(pred[0])
  # print(classIDx)
  # if i == 6:
  #   break

imageTest = np.expand_dims(imageTest, 0)
  pred = model.predict(imageTest)
  classIDx = np.argmax(pred[0])







history = model.fit(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = 10,
            epochs = 28,
            validation_steps = 10,
            verbose = 2, callbacks=[model_checkpoint_callback])

#import tensorflow.keras as keras

vis_model = keras.models.load_model(checkpoint_filepath)

model.summary()

vis_model.summary()

#### OLD execution ####
vis_model.summary()

for layer in reversed(vis_model.layers):
  if len(layer.output_shape) == 4:
    print("shape: ", layer.output_shape ,"layer name: ", layer.name)
    break

val_a, val_b = validation_generator[10]

validation_generator.batch_index

for i in validation_generator:
  idx = (validation_generator.batch_index - 1) * validation_generator.batch_size
  print(idx)

validation_generator[4]

validation_generator[10 : 10 + validation_generator.batch_size]

val_na, val_nb = validation_generator[2]

val_a[1].shape

print(val_b[1])
print(val_nb[1])

img_input = val_a[1].reshape(256,256,3)
input = cv2.resize(img_input, (256, 256))

print(img_input.shape, input.shape)

validation_strep_dir + '/' + validation_strep_fnames[10]

#img_path = validation_strep_dir + '/' + validation_strep_fnames[10]

import matplotlib.pyplot as plt


plt.imshow(val_a[0].reshape(256,256,3))
plt.show()

plt.imshow(val_na[0].reshape(256,256,3))
plt.show()

import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow import keras
import numpy as np
from PIL import Image
import os
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.applications import imagenet_utils
from tensorflow.keras.preprocessing.image import load_img

from tensorflow.keras.preprocessing.image import img_to_array
import matplotlib.pyplot as plt
import cv2

#######   Visualization ##########

import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow import keras
import numpy as np
from PIL import Image
import os
import matplotlib.pyplot as plt
import cv2


#species = ['Amblyomma', 'Dermacentor', 'Ixodes']
#label = ['no_pharyngitis', 'pharyngitis']
#species = ['pharyngitis', 'no_pharyngitis']

### loaded above ####
#model = keras.models.load_model(checkpoint_filepath)


img_path = validation_strep_dir + '/' + validation_strep_fnames[10]

new_image = cv2.imread(img_path)



# img = Image.open(img_path)
# img_resized = img.resize((256, 256,3))
# pixels = np.asarray(img_resized)  # convert image to array
# pixels = pixels.astype('float32')

# input = np.expand_dims(pixels, axis=0)

#img_input = val_a[1].reshape(256,256,3)



######################

# resized = cv2.resize(new_image, (256, 256))
# # load the input image from disk (in Keras/TensorFlow format) and
# # preprocess it
# image = load_img(img_path, target_size=(256, 256))
# image = img_to_array(image)
# image = np.expand_dims(image, axis=0)
# image = imagenet_utils.preprocess_input(image)
######################




img_input = val_a[1].reshape(256,256,3)
input1 = cv2.resize(img_input, (256, 256))
input = np.expand_dims(input1, axis=0)


preds = vis_model.predict(input)
i = np.argmax(preds[0])








print("the pred value is :", preds)
print(np.argmax(preds[0]))







# plt.imshow(image)
# plt.show()

# model.summary()

print(i)

val_a[1].shape[1]

img_input.shape[0]

import imutils

import cv2

label = str(i)
# draw the predicted label on the output image
#cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)
#cv2.putText(output, label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,0.8, (255, 255, 255), 2)


# Ensure all images have the same width before stacking
# new_image = imutils.resize(new_image, width=heatmap.shape[1])
# output = imutils.resize(output, width=heatmap.shape[1])

# display the original image and resulting heatmap and output image
# to our screen
output = np.vstack([new_image, heatmap, output])
#output = imutils.resize(output, height=700)
plt.imshow(heatmap)
plt.title("heatmap")
plt.show()


plt.imshow(new_image) # Use cv2.imshow to display the image
plt.title("new_image") # Set the title separately
plt.show()

#cv2.waitKey(0)

#raw = Image.open(train_strep_dir + '/' + train_strep_fnames[10])
raw = img_input
raw1 = raw.resize((256,256, 3))
#input = np.expand_dims(img_input, axis=0)
#raw1 = raw.resize((256,256))
# heatmap = cv2.resize(heatmap, (256, 256, 3))
# heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)
print(raw.shape)
#raw.reshape((256,256,3))

#image_test = cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB)
image_test = np.float32(cv2.resize(img_input, (224, 224)))
##image = np.expand_dims(image, axis=0)
print(image_test.shape)

with tf.GradientTape() as tape:
        last_conv_layer = model.get_layer('conv2d_163')
        iterate = tf.keras.models.Model([model.inputs], [model.output, last_conv_layer.output])
        model_out, last_conv_layer = iterate(input)
        class_out = model_out[:, np.argmax(model_out[0])]
        grads = tape.gradient(class_out, last_conv_layer)
        pooled_grads = K.mean(grads, axis=(0, 1))

heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)

heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)
print(heatmap.shape)

#heatmap = heatmap.reshape((5, 5))
heatmap = heatmap.reshape(heatmap.shape[1:])
plt.matshow(heatmap)
plt.show()

INTENSITY = 0.4

#raw = Image.open(img_path)
#raw = raw.resize((224,224))
raw = np.float32(cv2.resize(img_input, (256, 256)))
heatmap = cv2.resize(heatmap, (256, 256))
heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)


img = (heatmap * INTENSITY + np.array(raw))/255

# plt.imshow(raw)
# plt.show()

plt.imshow(img)
plt.show()

import numpy as np

ppred_test = model.predict(val_a)
if ppred_test > 0.6:
  print("Pharyngitis")
else:
  print("No Pharyngitis")



#model.save('./throat/incepv3_model.keras')

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()


plt.show()

#Load the trained TensorFlow model
import tensorflow as tf

#loaded_model = tf.keras.models.load_model("./throat/incepv3_model.keras")
#model.load_weights(checkpoint_filepath)
loaded_model = keras.models.load_model(checkpoint_filepath)

!pip install tf2onnx
!pip install onnx2pytorch

#Converting to tf2onnx Model
import tf2onnx

onnx_model, _ = tf2onnx.convert.from_keras(loaded_model)

#Converting to PyTorch Model

import onnx
from onnx2pytorch import ConvertModel

# Convert ONNX model to PyTorch
pytorch_model = ConvertModel(onnx_model)
pytorch_model

########  Test if connected to TPU #############

import tensorflow as tf
print("Tensorflow version " + tf.__version__)

try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  print(f'Running on a TPU w/{tpu.num_accelerators()["TPU"]} cores')
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.TPUStrategy(tpu)

state_dict = pytorch_model.state_dict()

state_dict

### GRAD CAM

import torch.nn as nn
import torchvision
import torch

model = torchvision.models.resnet50(pretrained=True)

num_ftrs = model.fc.in_features

model.fc = nn.Linear(num_ftrs, 2)

model.load_state_dict(state_dict)

#state_dict = torch.load('./model/best_model.pth', map_location=lambda storage, loc: storage)

import onnx
# from onnx.backend import prepare

# from onnx_tf.backend import prepare1

# from onnx2pytorch.operations import convert_op

from onnx_model.backend import prepare




####
# import onnx
# from onnx_tf.backend import prepare
# onnx_model = onnx.load('your_model.onnx')
# tf_rep = prepare(onnx_model)